{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Module 9 - (Automated) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we will be discussing *automated software testing*.\n",
    "\n",
    "We'll follow a [well-regarded tutorial](https://nedbatchelder.com/text/test3.html) by [Ned Batchelder](https://nedbatchelder.com/), a Python programmer and mentor with many well regarded conference talks, tutorials and posts. The below notebook serves mostly to contextualize Ned's post with what we've learned in the course, giving us a way to also practice what we've learned about Git and Object Oriented Programming.\n",
    "\n",
    "## Software Testing\n",
    "\n",
    "Let's deconstruct the phrase to start -- *software testing* is a method of checking that code you have written functions as intended. We call each independent example (with its expected result) a *test case*, and often refer to the set of all examples as a *test suite*.\n",
    "*Automated* software testing refers to the practice of *programmatically* defining the test cases you write in a way that they can be repeatedly, automatically *checked* to ensure they produce the expected results. In contrast, manual testing refers to authoring of tests which must be manually run, or whose results must be manually checked against the expected outcomes.\n",
    "\n",
    "Automated software testing is an extremely important topic. This is particularly true for software developers, but for you as *non*-software developers, it's important to consider some reasons why it is still extremely useful.\n",
    "\n",
    "* You may author a script or program which processes some data and produces a report -- if the input data format changes, or you wish to change the generated report, you will want assurances that your report still is correct.\n",
    "\n",
    "* You may attempt to write a particularly tricky piece of code to solve a problem, and as part of doing so will want to ensure each time you make a change that your program is getting successively more correct\n",
    "\n",
    "* You may encounter a library you wish to use, and need a way to evaluate how likely it is to work well. Looking at or understanding its test suite can help you to do so.\n",
    "\n",
    "* You may encounter a library you wish to contribute to, and as part of doing so will need to understand how to run the library's existing tests, or author new ones for the change you're making\n",
    "\n",
    "Overall, testing is a tool you can use to make sure your code works the way you expect, and does so even as you continue to work on it. This tutorial should help demonstrate.\n",
    "\n",
    "As above, you're encouraged to follow Ned's full tutorial as-is after class, should you be interested, but we'll proceed with a version which follows Ned's examples in a looser way.\n",
    "\n",
    "We'll do so as an in-class exercise, following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Repository\n",
    "\n",
    "Let's start by creating a place to put our work, which we can do by recalling our Git & GitHub lecture.\n",
    "\n",
    "Create a GitHub repository which you'll push to for each step in the below. Remember that you can do so in the GitHub Desktop application we installed previously. You also may do so [directly in the GitHub application](https://github.com/new).\n",
    "\n",
    "You may call this repository what you'd like, but as we'll be writing some code for a simple stock portfolio, I'll call mine `stock-portfolio`.\n",
    "\n",
    "If you finish doing so, feel free to add a `README.md` file to your repository explaining that it will be used as a way to learn automated testing by creating a simple stock portfolio. You may want to also link to Ned's post within it, giving credit to his tutorial.\n",
    "\n",
    "If at any point in the below you wish to catch up, you can refer to code in Ned's post to catch up. Try however to do so only if stuck, not to copy the answers Ned has provided to some of the below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, Our Program\n",
    "\n",
    "Let's first start, following Ned's example, by creating a type representing a stock portfolio.\n",
    "\n",
    "Create a file in your repository called `portfolio.py` by using VSCode from GitHub Desktop, as we did a few weeks ago.\n",
    "\n",
    "Within it, define a new class called `Portfolio` which:\n",
    "\n",
    "* has a method called `buy`, which adds a new stock to the portfolio, taking 3 arguments\n",
    "\n",
    "    * `name`, a `str`, the symbol of the stock which is being bought\n",
    "    \n",
    "    * `shares`, an `int`, the quantity which is being bought\n",
    "    \n",
    "    * `price`, a `float`, the price paid per share\n",
    "    \n",
    "* and has a second method called `cost` which returns a `float`, the total cost paid for all stocks in the portfolio\n",
    "\n",
    "Consider that to implement the `cost` method, you'll need to be storing the purchases made each time the `buy` method is called somewhere. You may do so by any means convenient to you.\n",
    "\n",
    "**Commit** this file to your repository and push it to GitHub using GitHub Desktop, with a suitable commit message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Test: Interactive\n",
    "\n",
    "As Ned says:\n",
    "\n",
    "> For our first test, we just run it manually in a Python prompt. This is where most programmers start with testing: play around with the code and see if it works.\n",
    "\n",
    "Let's see a new way to do so, via a shell running from *within* Jupyter Lab. We've used a Python REPL within Google Cloud Shell before, but we can get a local one on your own computer by running `File > New > Terminal`.\n",
    "\n",
    "Once you've done so, use `cd` to change to the directory where you've stored your repository. If you let GitHub Desktop create this for you, it will likely be in your Documents folder, or wherever you told it to place the repository.\n",
    "\n",
    "You can now run `python3` from within that directory, followed by `from portfolio import Portfolio` to import your new class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenarios\n",
    "\n",
    "There are a number of scenarios we may test in the above. For example:\n",
    "\n",
    "* The cost of an empty portfolio should be 0\n",
    "* Buying 100 shares of IBM at `$176.48` should give a portfolio that costs `$17648.00`\n",
    "* Then buying 100 shares of HPQ at `$36.15` should result in a final cost of `$21263.00`\n",
    "\n",
    "Does your code produce the correct result in the above scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Issues\n",
    "\n",
    "Again, following Ned's example:\n",
    "\n",
    "> Running it like this, we can see that it’s right. An empty portfolio has a cost of zero. We buy one stock, and the cost is the price times the shares. Then we buy another, and the cost has gone up as it should.\n",
    "\n",
    "> This is good, we’re testing the code. Some developers wouldn’t have even taken this step! But it’s bad because it’s not repeatable. If tomorrow we make a change to this code, it’s hard to make sure that we’ll run the same tests and cover the same conditions that we did today.\n",
    "\n",
    "> It’s also labor intensive: we have to type these function calls each time we want to test the class. And how do we know the results are right? We have to carefully examine the output, and get out a calculator, and see that the answer is what we expect.\n",
    "\n",
    "## An Improvement: A Standalone Test Script\n",
    "\n",
    "We can improve on things by taking the above scenarios and *codifying* them in a script that we run -- a *test script*, which contains the aforementioned scenarios, and which we can run any time we'd like to re-run the same examples.\n",
    "\n",
    "Paste the below into a new file called `test_portfolio.py`, which you should then save, commit and push:\n",
    "\n",
    "```python\n",
    "# test_portfolio.py\n",
    "from portfolio import Portfolio\n",
    "\n",
    "p = Portfolio()\n",
    "print(f\"Empty portfolio cost: {p.cost()}\")\n",
    "p.buy(\"IBM\", 100, 176.48)\n",
    "print(f\"With 100 IBM @ 176.48: {p.cost()}\")\n",
    "p.buy(\"HPQ\", 100, 36.15)\n",
    "print(f\"With 100 HPQ @ 36.15: {p.cost()}\")\n",
    "```\n",
    "\n",
    "Confirm that when you run this file in a shell, now with `python3 test_portfolio.py`, you get the expected output. Do so multiple times and observe of course you get the same output.\n",
    "\n",
    "At this point, we have a *repeatable* test we can run over and over again, and do so quicker than typing out the examples at a REPL.\n",
    "\n",
    "### Some Issues\n",
    "\n",
    "We still have an important remaining issue however, which is that we don't know the answers we print are *correct* unless we hand-check them. In other words, we are printing what our portfolio object calculates for us, but we have not compared the output in a way that makes it easy to see whether the output is the correct one.\n",
    "\n",
    "## An Improvement: Adding Expected Results\n",
    "\n",
    "We can solve the above by including the correct answers in our output, such that they're comparable to the ones our `Portfolio` class emits.\n",
    "\n",
    "Modify the test file to contain the below:\n",
    "\n",
    "```python\n",
    "# test_portfolio.py\n",
    "from portfolio import Portfolio\n",
    "\n",
    "p = Portfolio()\n",
    "print(f\"Empty portfolio cost: {p.cost()}, should be 0.0\")\n",
    "p.buy(\"IBM\", 100, 176.48)\n",
    "print(f\"With 100 IBM @ 176.48: {p.cost()}, should be 17648.0\")\n",
    "p.buy(\"HPQ\", 100, 36.15)\n",
    "print(f\"With 100 HPQ @ 36.15: {p.cost()}, should be 21263.0\")\n",
    "```\n",
    "\n",
    "where we now have included the correct expected values for comparison.\n",
    "\n",
    "### Some Issues\n",
    "\n",
    "Still, we are left having to manually look at the two answers and compare them to each other to see if any are broken.\n",
    "\n",
    "We should instead be able to have our computer check the answers for us.\n",
    "\n",
    "To do so, we'll introduce a statement you've likely seen in the homework files previously, the `assert` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "assert 2 == 7, \"2 does not equal 7!\"  # if the condition is false then the statement beside it will be thrown, its like rasing an error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Improvement: Automated Checking of Expected Values\n",
    "\n",
    "Modify the test file to now use the assert statement as below:\n",
    "\n",
    "```python\n",
    "# test_portfolio.py\n",
    "from portfolio import Portfolio\n",
    "\n",
    "p = Portfolio()\n",
    "print(f\"Empty portfolio cost: {p.cost()}, should be 0.0\")\n",
    "assert p.cost() == 0.0\n",
    "p.buy(\"IBM\", 100, 176.48)\n",
    "print(f\"With 100 IBM @ 176.48: {p.cost()}, should be 17648.0\")\n",
    "assert p.cost() == 17648.0\n",
    "p.buy(\"HPQ\", 100, 36.15)\n",
    "print(f\"With 100 HPQ @ 36.15: {p.cost()}, should be 21263.0\")\n",
    "assert p.cost() == 21263.0\n",
    "```\n",
    "\n",
    "Intentionally break your portfolio program now, and notice what output you get from this test script.\n",
    "\n",
    "### Some Issues\n",
    "\n",
    "While we now have an automated program we can use (and add to) to ensure that our portfolio is functioning correctly, there are still some issues we can improve on.\n",
    "\n",
    "Again quoting Ned:\n",
    "\n",
    "> First, all the successful tests clutter up the output. You may think it’s good to see all your successes, but it’s not good if they obscure failures. Second, when an assertion fails, it raises an exception, which ends our program [...] We can only see a single failure, then the rest of the program is skipped, and we don’t know the results of the rest of the tests. This limits the amount of information our tests can give us.\n",
    "\n",
    "Note this as above when you broke your program, or similarly if you intentionally change the value a test is expecting.\n",
    "\n",
    "We could imagine making our *test script* smarter to deal with some of these issues. But others writing tests encounter these same issues and have the same desires -- and so, have built what are called *test frameworks* we can use to improve further on the output we get, fixing these issues and getting us even more functionality in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## py.test\n",
    "\n",
    "The test framework most commonly used is known as [pytest](https://docs.pytest.org/en/6.2.x/).\n",
    "\n",
    "It is installed alongside anaconda, so if you run the below (which recall runs an external program) you should see its version output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytest 6.1.1\n"
     ]
    }
   ],
   "source": [
    "!pytest --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytest` takes many arguments, many useful in different testing scenarios but we'll only use a few of them. Specifically, using it with no arguments will attempt to *discover* all of the tests in the directory you run it in.\n",
    "\n",
    "Before we can *use* `pytest`, we must write our tests in a way `pytest` will understand how to run them. Once we've done so, `pytest` will *discover* tests we have written and run each one of them in the way we want.\n",
    "\n",
    "As Ned says:\n",
    "\n",
    "> Test frameworks all look weird when you first learn about them. Running tests is different than running a program, so they have different conventions for how to write tests, and how they get run\n",
    "\n",
    "Testing with `py.test` has two key components:\n",
    "\n",
    "* each individual test case you want to run independently gets put in a *function* which must be named `test_<something>`, where the name should somehow describe what you are testing\n",
    "* each of these test cases should be put in a *module* called `test_<something>.py`, where the name should somehow describe the collection of test cases all together. Here we will have only one test file, the `test_portfolio.py` file we've started with.\n",
    "\n",
    "Modify the `test_portfolio.py` file, commenting out its contents if you'd like, and replace them with our first `py.test` test:\n",
    "\n",
    "```python\n",
    "from portfolio import Portfolio\n",
    "\n",
    "def test_buy_one_stock():\n",
    "    p = Portfolio()\n",
    "    p.buy(\"IBM\", 100, 176.48)\n",
    "    assert p.cost() == 17648.0\n",
    "```\n",
    "\n",
    "You'll notice the similarity this has with our manually-written script, where the only real difference is that for the isolation reasons mentioned, we put the example in a function, rather than globally.\n",
    "\n",
    "Now run the above. You can do so by running `pytest` from a shell as mentioned above.\n",
    "\n",
    "Once you've observed the passing test, commit and push this test, then see if you can convert the full example above into additional tests.\n",
    "\n",
    "You should intentionally break either your code or tests and observe how py.test shows its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation\n",
    "\n",
    "As Ned points out:\n",
    "\n",
    "> Test isolation means that each of your tests is unaffected by every other test. This is good because it makes your tests more repeatable, and they are clearer about what they are testing. It also means that if a test fails, you don’t have to think about all the conditions and data created by earlier tests: running just that one test will reproduce the failure.\n",
    "\n",
    "For example, we may have forgotten the *edge case* of an empty portfolio, leading to incorrect behavior in this case. Having a separate test that covers this scenario means if we've done so, we'll notice that the non-empty portfolio tests pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `py.test -k`\n",
    "\n",
    "It's often useful to be able to run only a subset of tests \"either because you want to run just one test for debugging, or for a faster run of only the tests you are interested in\".\n",
    "\n",
    "`pytest` has a `-k` option in order to do so. It takes as an argument a string which will be matched against your test case name.\n",
    "\n",
    "Try using the argument to run just a test against the empty portfolio case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Test Cases\n",
    "\n",
    "Try writing another test to ensure that two separate portfolio instances can be created, and their cost is independent of each other. In other words, creating a portfolio and buying a stock should not affect a second portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Edge Cases – `pytest.raises`\n",
    "\n",
    "Continuing with our source tutorial -- what happens if we wish to test a case which raises an exception? Specifically, what does our `Portfolio` object do if we don't give it the right arguments?\n",
    "\n",
    "Write a test which calls `portfolio.buy` with too few arguments.\n",
    "\n",
    "Observe that when you run `py.test`, the test is reported as an error.\n",
    "\n",
    "If we wish to write a test which we *expect* to raise an exception (and want to fail if we do not), `import pytest`, which will give you a *context manager* (something we use with the `with` statement) called `pytest.raises`.\n",
    "\n",
    "Use `pytest.raises` with the type of exception you expect this method to raise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Behavior\n",
    "\n",
    "We'll here branch off a bit from Ned's tutorial, which you are again encouraged to follow afterwards.\n",
    "\n",
    "But let's instead use this opportunity to demonstrate a number of additional pieces of functionality we can add to our `Portfolio`, and practice writing tests for this additional functionality.\n",
    "\n",
    "Commit any changes to your repository, push to GitHub, and then here are 3 additional pieces of functionality to implement, along with tests:\n",
    "\n",
    "* Add a `Shares` class which represents the symbol, number of shares, and price paid. Change the `.buy` method to take as an argument an instance of this new class. How do our tests ensure that we've correctly made the change to our code?\n",
    "\n",
    "* Add a `.sell` method to your `Portfolio` class which sells a specific number of shares of a given symbol. Consider what this method should do in various edge cases, such as if there aren't enough shares of the stock in the portfolio, or the number of sold shares is negative.\n",
    "\n",
    "* Add a `.save` method to your `Portfolio` class which takes a `sqlite3.Connection` and saves the contents of the portfolio into a `stocks` table. Remember that you can use a `sqlite3.connect(\":memory:\")` in-memory database. How would you test this method?\n",
    "\n",
    "* Make your `Portfolio` objects allow an initial set of \"pre-purchased\" shares. What form should this argument take?\n",
    "\n",
    "* Define `Portfolio.__eq__`, allowing us to compare portfolio objects to each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub Actions\n",
    "\n",
    "We mentioned [GitHub Actions](https://github.com/features/actions) briefly when learning about Git and GitHub.\n",
    "\n",
    "GitHub Actions gives us ways to have the tests we've written *run automatically* every time we push to our repository.\n",
    "\n",
    "There is an extremely wide ecosystem of \"actions\" at your disposal, some written by GitHub, many written by open source developers.\n",
    "\n",
    "Configuring GitHub actions is done using a language known as *YAML*. You can learn the basics of YAML syntax [using a tutorial here](https://learnxinyminutes.com/docs/yaml/), but let's just see what is needed to get the tests you've written running in GitHub actions.\n",
    "\n",
    "Copy the below into a file called `.github/workflows/tests.yml` within your repository (note the leading `.`, and that you will need to create these directories), and commit and push to GitHub:\n",
    "\n",
    "```yaml\n",
    "name: Continuous Integration\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: 3.9\n",
    "      - name: Install pytest\n",
    "        run: python3 -m pip install pytest\n",
    "      - name: Test with pytest\n",
    "        run: pytest -vv\n",
    "```\n",
    "\n",
    "If you've done so correctly, after you push, go to `https://github.com/<yourusername>/stock-portfolio/actions` (or click the Actions tab) and you should see a test run which has run your tests!\n",
    "\n",
    "We call this process of continuously testing and fixing changes to a codebase *continuous integration*.\n",
    "\n",
    "As a final step, GitHub will automatically create a *badge* -- which is essentially a small image -- that shows whether your tests are passing at any given point in time.\n",
    "\n",
    "You can find that badge for your own repository at `https://github.com/<yourusername>/stock-portfolio/workflows/Continuous%20Integration/badge.svg`.\n",
    "\n",
    "First go to that link and you should see the image. Then you can embed that image in a `README.md` file in your repository to show off that your tests are passing.\n",
    "\n",
    "The syntax for doing so in Markdown is:\n",
    "\n",
    "```markdown\n",
    "# Stock Portfolio\n",
    "\n",
    "![](https://github.com/<yourusername>/stock-portfolio/workflows/Continuous%20Integration/badge.svg)\n",
    "\n",
    "A simple stock portfolio testing example, using Ned Batchelder's testing tutorial.\n",
    "```\n",
    "\n",
    "GitHub actions is extremely powerful. You can find more information and tutorials [in its documentation](https://docs.github.com/en/actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Example Test Suites\n",
    "\n",
    "* [pandas](https://github.com/pandas-dev/pandas/tree/master/pandas/tests)\n",
    "* [numpy](https://github.com/numpy/numpy/tree/main/numpy/tests)\n",
    "* [pytest's own suite](https://github.com/pytest-dev/pytest/tree/main/testing)\n",
    "\n",
    "The above test suites may not be the easiest to read, especially until we've covered these libraries themselves. But often a test suite can be used as a way to:\n",
    "\n",
    "* assess the quality of a library based on the presence and comprehensivity of its tests\n",
    "\n",
    "* learn how to use a library by utilizing the examples the test suite contains\n",
    "\n",
    "* practice contributing to a library by modifying it and running its test suite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
